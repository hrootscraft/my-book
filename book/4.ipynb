{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import pickle\n",
    "\n",
    "percent = 0\n",
    "df = pd.read_csv(\"processed.csv\")\n",
    "\n",
    "X = df.drop(\"Churn Label\", axis=1)\n",
    "y = df[\"Churn Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=20\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "del X_train[\"Unnamed: 0\"]\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "del X_test[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "def predict_new(model_name, model, X_train, y_train, X_test, y_test):\n",
    "    col1, col2, col3, col4, col5 = st.columns(5)\n",
    "    with col1:\n",
    "        senior_citizen = st.selectbox(\"Are they a senior citizen?\", [\"Yes\", \"No\"])\n",
    "    with col2:\n",
    "        partner = st.selectbox(\"Do they have a partner?\", [\"Yes\", \"No\"])\n",
    "    with col3:\n",
    "        dependent = st.selectbox(\"Do they have a dependent?\", [\"Yes\", \"No\"])\n",
    "    with col4:\n",
    "        phone_services = st.selectbox(\n",
    "            \"Have they subscibed to phone services?\", [\"Yes\", \"No\"]\n",
    "        )\n",
    "    with col5:\n",
    "        internet_services = st.selectbox(\n",
    "            \"Have they subscibed to internet services?\", [\"DSL\", \"Fiber optic\", \"No\"]\n",
    "        )\n",
    "\n",
    "    col11, col12, col13, col14, col15 = st.columns(5)\n",
    "    with col11:\n",
    "        online_security = st.selectbox(\n",
    "            \"Have they subscibed to online security?\",\n",
    "            [\"Yes\", \"No\", \"No internet services\"],\n",
    "        )\n",
    "    with col12:\n",
    "        online_backup = st.selectbox(\n",
    "            \"Have they subscibed to online backup?\",\n",
    "            [\"Yes\", \"No\", \"No internet services\"],\n",
    "        )\n",
    "    with col13:\n",
    "        device_protection = st.selectbox(\n",
    "            \"Have they subscibed to device protection?\",\n",
    "            [\"Yes\", \"No\", \"No internet services\"],\n",
    "        )\n",
    "    with col14:\n",
    "        tech_support = st.selectbox(\n",
    "            \"Have they subscibed to tech support?\",\n",
    "            [\"Yes\", \"No\", \"No internet services\"],\n",
    "        )\n",
    "    with col15:\n",
    "        contract = st.selectbox(\n",
    "            \"What type of contract do they have?\",\n",
    "            [\"Month-to-month\", \"Two Year\", \"One Year\"],\n",
    "        )\n",
    "\n",
    "    col21, col22 = st.columns(2)\n",
    "    with col21:\n",
    "        paperless_billing = st.selectbox(\n",
    "            \"Do they have paperless billing\", [\"Yes\", \"No\"]\n",
    "        )\n",
    "    with col22:\n",
    "        payment_method = st.selectbox(\n",
    "            \"What's their main payment method?\",\n",
    "            [\n",
    "                \"Mailed check\",\n",
    "                \"Electronic check\",\n",
    "                \"Bank transfer (automatic)\",\n",
    "                \"Credit card (automatic)\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    col23, col24, col25 = st.columns(3)\n",
    "    with col23:\n",
    "        months = st.number_input(\n",
    "            \"Enter number of months they were active\", step=1, min_value=0\n",
    "        )\n",
    "    with col24:\n",
    "        monthly_payment = st.number_input(\n",
    "            \"Enter their monthly payment\", step=1, min_value=0\n",
    "        )\n",
    "    with col25:\n",
    "        Total_payment = st.number_input(\n",
    "            \"Enter the total amount they paid\", step=1, min_value=0\n",
    "        )\n",
    "\n",
    "    if st.button(\"Predict Now\"):\n",
    "        data = []\n",
    "        data.append(\n",
    "            [\n",
    "                months,\n",
    "                monthly_payment,\n",
    "                Total_payment,\n",
    "                senior_citizen,\n",
    "                partner,\n",
    "                dependent,\n",
    "                phone_services,\n",
    "                internet_services,\n",
    "                online_security,\n",
    "                online_backup,\n",
    "                device_protection,\n",
    "                tech_support,\n",
    "                contract,\n",
    "                paperless_billing,\n",
    "                payment_method,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        all_cols = [\n",
    "            \"Tenure Months\",\n",
    "            \"Monthly Charges\",\n",
    "            \"Total Charges\",\n",
    "            \"Churn Label\",\n",
    "            \"Senior Citizen_No\",\n",
    "            \"Senior Citizen_Yes\",\n",
    "            \"Partner_No\",\n",
    "            \"Partner_Yes\",\n",
    "            \"Dependents_No\",\n",
    "            \"Dependents_Yes\",\n",
    "            \"Phone Service_No\",\n",
    "            \"Phone Service_Yes\",\n",
    "            \"Internet Service_DSL\",\n",
    "            \"Internet Service_Fiber optic\",\n",
    "            \"Internet Service_No\",\n",
    "            \"Online Security_No\",\n",
    "            \"Online Security_No internet service\",\n",
    "            \"Online Security_Yes\",\n",
    "            \"Online Backup_No\",\n",
    "            \"Online Backup_No internet service\",\n",
    "            \"Online Backup_Yes\",\n",
    "            \"Device Protection_No\",\n",
    "            \"Device Protection_No internet service\",\n",
    "            \"Device Protection_Yes\",\n",
    "            \"Tech Support_No\",\n",
    "            \"Tech Support_No internet service\",\n",
    "            \"Tech Support_Yes\",\n",
    "            \"Contract_Month-to-month\",\n",
    "            \"Contract_One year\",\n",
    "            \"Contract_Two year\",\n",
    "            \"Paperless Billing_No\",\n",
    "            \"Paperless Billing_Yes\",\n",
    "            \"Payment Method_Bank transfer (automatic)\",\n",
    "            \"Payment Method_Credit card (automatic)\",\n",
    "            \"Payment Method_Electronic check\",\n",
    "            \"Payment Method_Mailed check\",\n",
    "        ]\n",
    "\n",
    "        df1 = pd.DataFrame(\n",
    "            data,\n",
    "            columns=[\n",
    "                \"Tenure Months\",\n",
    "                \"Monthly Charges\",\n",
    "                \"Total Charges\",\n",
    "                \"Senior Citizen\",\n",
    "                \"Partner\",\n",
    "                \"Dependents\",\n",
    "                \"Phone Service\",\n",
    "                \"Internet Service\",\n",
    "                \"Online Security\",\n",
    "                \"Online Backup\",\n",
    "                \"Device Protection\",\n",
    "                \"Tech Support\",\n",
    "                \"Contract\",\n",
    "                \"Paperless Billing\",\n",
    "                \"Payment Method\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        with open(\"ohe.pkl\", \"rb\") as pickle_file:\n",
    "            ohe = pickle.load(pickle_file)\n",
    "        cat_columns = [cname for cname in df1.columns if df1[cname].dtype == \"object\"]\n",
    "        train_X_encoded = pd.DataFrame(ohe.fit_transform(df1[cat_columns]))\n",
    "        train_X_encoded.columns = ohe.get_feature_names_out(cat_columns)\n",
    "        df1 = df1.drop(cat_columns, axis=1).copy()\n",
    "        df2 = pd.concat([df1, train_X_encoded], axis=1)\n",
    "\n",
    "        for c in all_cols:\n",
    "            if c not in df2.columns:\n",
    "                df2[c] = 0\n",
    "\n",
    "        df3 = df2.copy()\n",
    "        df3 = df3.drop(\"Churn Label\", axis=1).copy()\n",
    "        df_col = list(df.columns)\n",
    "        df_col.remove(\"Churn Label\")\n",
    "        df3 = df3.reindex(columns=df_col)\n",
    "        df3 = df3.drop(\"Unnamed: 0\", axis=1)\n",
    "        print(df3.columns)\n",
    "        pred = model.predict_proba(df3)\n",
    "        print(pred)\n",
    "        st.write(\"The probability of customer churning is \", pred[0][1])\n",
    "\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    accuracy = history.history[\"accuracy\"]\n",
    "    val_accuracy = history.history[\"val_accuracy\"]\n",
    "    epochs = range(len(history.history[\"loss\"]))\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        # Plot loss\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, loss, label=\"training_loss\")\n",
    "        plt.plot(epochs, val_loss, label=\"val_loss\")\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend()\n",
    "        st.pyplot(plt)\n",
    "    with col2:\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
    "        plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend()\n",
    "        st.pyplot(plt)\n",
    "\n",
    "\n",
    "def run(X_train, y_train, X_test, y_test, d):\n",
    "    model = st.selectbox(\n",
    "        \"Which models do you want to run?\",\n",
    "        [\n",
    "            \"\",\n",
    "            \"Ensemble\",\n",
    "            \"Logistic Regression\",\n",
    "            \"SVM\",\n",
    "            \"Random Forest\",\n",
    "            \"KNN\",\n",
    "            \"xgboost\",\n",
    "            \"Lightboost\",\n",
    "            \"Neural Network\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    def evaluate(typ, model, X_train, y_train, x_test, y_test):\n",
    "        if typ == \"nn\":\n",
    "            pred = model.predict(X_test)\n",
    "            pred = [1 if x > 0.5 else 0 for x in pred]\n",
    "            pred = pd.Series(pred)\n",
    "            pred = pred.astype(int)\n",
    "        else:\n",
    "            pred = model.predict(x_test)\n",
    "\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            cm = confusion_matrix(y_test, pred)\n",
    "            ax = plt.subplot()\n",
    "            sns.heatmap(cm, annot=True, ax=ax, fmt=\"g\")\n",
    "            ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "            ax.xaxis.set_label_position(\"top\")\n",
    "            ax.xaxis.set_ticklabels([\"Not Churned\", \"Churned\"], fontsize=15)\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.set_ylabel(\"True\", fontsize=20)\n",
    "            ax.yaxis.set_ticklabels([\"Not Churned\", \"Churned\"], fontsize=15)\n",
    "            st.pyplot(plt)\n",
    "        with col2:\n",
    "            fpr, tpr, threshold = roc_curve(y_test, pred)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.title(\"Receiver Operating Characteristic\")\n",
    "            plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.plot([0, 1], [0, 1], \"r--\")\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            st.pyplot(plt)\n",
    "        return round(f1_score(y_test, pred, average=\"weighted\"), 5)\n",
    "\n",
    "    st.write(\"Running model \", model)\n",
    "\n",
    "    if len(model) > 0:\n",
    "        if model == \"Logistic Regression\":\n",
    "            par = st.selectbox(\n",
    "                \"What model parameters do you want?\",\n",
    "                (\"\", \"Recommended\", \"Custom\"),\n",
    "            )\n",
    "            if len(par) > 0:\n",
    "                if par == \"Recommended\":\n",
    "                    c = 1.0\n",
    "                    solver = \"liblinear\"\n",
    "                    best_params = d[\"Logistic Regression\"]\n",
    "                else:\n",
    "                    c = st.slider(\n",
    "                        \"What value of c do you want to use?\",\n",
    "                        min_value=0.001,\n",
    "                        max_value=10.0,\n",
    "                        step=0.1,\n",
    "                    )\n",
    "                    st.write(\n",
    "                        \"liblinear tries for l1,l2 and sage tries for ElasticNet, l1,l2,none\"\n",
    "                    )\n",
    "                    solver = st.selectbox(\n",
    "                        \"Which solver do you want to use?\", [\"liblinear\", \"saga\"]\n",
    "                    )\n",
    "                    best_params = {\"C\": c, \"solver\": solver}\n",
    "                lr = LogisticRegression(**best_params, class_weight=\"balanced\")\n",
    "                lr.fit(X_train, y_train)\n",
    "                lr_f1 = evaluate(\"lr\", lr, X_train, y_train, X_test, y_test)\n",
    "                st.write(\"Model trained with an F1 score of\", lr_f1)\n",
    "                predict_new(\"lr\", lr, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"SVM\":\n",
    "            par = st.selectbox(\n",
    "                \"What model parameters do you want?\",\n",
    "                (\"\", \"Recommended\", \"Custom\"),\n",
    "            )\n",
    "            if len(par) > 0:\n",
    "                if par == \"Recommended\":\n",
    "                    best_params = d[\"SVM\"]\n",
    "                else:\n",
    "                    c = st.slider(\n",
    "                        \"What value of c do you want to use?\",\n",
    "                        min_value=0.001,\n",
    "                        max_value=10.0,\n",
    "                        step=0.1,\n",
    "                    )\n",
    "                    gamma = st.slider(\n",
    "                        \"What value of gamma do you want to use?\",\n",
    "                        min_value=0.001,\n",
    "                        max_value=1.0,\n",
    "                        step=0.0001,\n",
    "                    )\n",
    "                    st.write(\n",
    "                        \"liblinear tries for l1,l2 and sage tries for ElasticNet, l1,l2,none\"\n",
    "                    )\n",
    "                    kernel = st.selectbox(\n",
    "                        \"What kernel value do you want to use?\",\n",
    "                        [\"rbf\", \"poly\", \"linear\", \"sigmoid\"],\n",
    "                    )\n",
    "                    best_params = {\"C\": c, \"gamma\": gamma, \"kernel\": kernel}\n",
    "                svm = SVC(**best_params)\n",
    "                svm.fit(X_train, y_train)\n",
    "                print(\"Model fit\")\n",
    "                svm_f1 = evaluate(\"svm\", svm, X_train, y_train, X_test, y_test)\n",
    "                st.write(\"Model trained with an F1 score of\", svm_f1)\n",
    "                print(svm_f1)\n",
    "                predict_new(\"svm\", svm, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"Random Forest\":\n",
    "            best_params = d[\"Random Forest\"]\n",
    "            rfc = RandomForestClassifier(**best_params)\n",
    "            rfc.fit(X_train, y_train)\n",
    "            rf_f1 = evaluate(\"rf\", rfc, X_train, y_train, X_test, y_test)\n",
    "            st.write(\"Model trained with an F1 score of\", rf_f1)\n",
    "            predict_new(\"rfc\", rfc, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"KNN\":\n",
    "            par = st.selectbox(\n",
    "                \"What kind of model parameters do you want?\",\n",
    "                (\"\", \"Recommended\", \"Custom\"),\n",
    "            )\n",
    "            if len(par) > 0:\n",
    "                if par == \"Recommended\":\n",
    "                    n = 7\n",
    "                else:\n",
    "                    n = st.slider(\"What value of n?\", min_value=1, max_value=20, step=1)\n",
    "                knn = KNeighborsClassifier(n_neighbors=n)\n",
    "                knn.fit(X_train, y_train)\n",
    "                knn_f1 = evaluate(\"knn\", knn, X_train, y_train, X_test, y_test)\n",
    "                st.write(\"Model trained with an F1 score of\", knn_f1)\n",
    "                predict_new(\"knn\", knn, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"xgboost\":\n",
    "            best_params = d[\"xgboost\"]\n",
    "            xgb_m = xgb.XGBClassifier(**best_params)\n",
    "            xgb_m.fit(X_train, y_train)\n",
    "            xgb_f1 = evaluate(\"xgb\", xgb_m, X_train, y_train, X_test, y_test)\n",
    "            st.write(\"Model trained with an F1 score of\", xgb_f1)\n",
    "            predict_new(\"xgb_m\", xgb_m, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"Lightboost\":\n",
    "            best_params = d[\"Lightboost\"]\n",
    "            lgb = lightgbm.LGBMClassifier(**best_params)\n",
    "            lgb.fit(X_train, y_train)\n",
    "            lgb_f1 = evaluate(\"lgb\", lgb, X_train, y_train, X_test, y_test)\n",
    "            st.write(\"Model trained with an F1 score of\", lgb_f1)\n",
    "            predict_new(\"lgb\", lgb, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"Ensemble\":\n",
    "            lr = LogisticRegression(**d[\"Logistic Regression\"])\n",
    "            svm = SVC(**d[\"SVM\"])\n",
    "            lgb = lightgbm.LGBMClassifier(**d[\"Lightboost\"])\n",
    "            rfc = RandomForestClassifier(**d[\"Random Forest\"])\n",
    "            xgb_m = xgb.XGBClassifier(**d[\"xgboost\"])\n",
    "            knn = KNeighborsClassifier(n_neighbors=6)\n",
    "            estimators = [\n",
    "                (\"lr\", lr),\n",
    "                (\"svm\", svm),\n",
    "                (\"rfc\", rfc),\n",
    "                (\"xgb_m\", xgb_m),\n",
    "                (\"knn\", knn),\n",
    "            ]\n",
    "            ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
    "            ensemble.fit(X_train, y_train)\n",
    "            pred = ensemble.predict(X_test)\n",
    "            ensemble_f1 = evaluate(\n",
    "                \"ensemble\", ensemble, X_train, y_train, X_test, y_test\n",
    "            )\n",
    "            st.write(\"Model trained with an F1 score of\", ensemble_f1)\n",
    "            predict_new(\"ensemble\", ensemble_f1, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if model == \"Neural Network\":\n",
    "            input_layer = Input(shape=(X_train.shape[1],))\n",
    "            par = st.selectbox(\n",
    "                \"What kind of model parameters do you want?\",\n",
    "                (\"\", \"Recommended\", \"Custom\"),\n",
    "            )\n",
    "            if len(par) > 0:\n",
    "                if par == \"Recommended\":\n",
    "                    dense_layer_1 = Dense(100, activation=\"sigmoid\")(input_layer)\n",
    "                    dense_layer_2 = Dense(100, activation=\"sigmoid\")(dense_layer_1)\n",
    "                    output_layer = Dense(1, activation=\"sigmoid\")(dense_layer_2)\n",
    "                    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "                    epochs = 10\n",
    "                else:\n",
    "                    layers = st.number_input(\n",
    "                        \"Enter the number of hidden layers\",\n",
    "                        step=1,\n",
    "                        min_value=1,\n",
    "                        max_value=10,\n",
    "                    )\n",
    "                    epochs = st.number_input(\n",
    "                        \"Enter the number of epochs\", step=1, min_value=2, max_value=50\n",
    "                    )\n",
    "                    layers = int(layers)\n",
    "                    units = [0] * layers\n",
    "                    activation = [\"\"] * layers\n",
    "\n",
    "                    for i in range(int(layers)):\n",
    "                        col1, col2 = st.columns(2)\n",
    "                        ustring = \"Enter number of units at hidden layer \" + str(i + 1)\n",
    "                        astring = (\n",
    "                            \"Enter activation function before hidden layer \"\n",
    "                            + str(i + 1)\n",
    "                        )\n",
    "                        with col1:\n",
    "                            ui = int(st.number_input(ustring, key=i, step=1))\n",
    "                        with col2:\n",
    "                            ai = st.selectbox(\n",
    "                                astring, (\"\", \"sigmoid\", \"relu\", \"tanh\"), key=i\n",
    "                            )\n",
    "                        units[i] = ui\n",
    "                        activation[i] = ai\n",
    "                    print(activation)\n",
    "                    if activation[-1] != \"\" and units[-1] != 0:\n",
    "                        model = Sequential()\n",
    "                        for i in range(layers):\n",
    "                            if i == 0:\n",
    "                                model.add(\n",
    "                                    Dense(\n",
    "                                        units[i],\n",
    "                                        activation[i],\n",
    "                                        input_shape=(X_train.shape[1],),\n",
    "                                    )\n",
    "                                )\n",
    "                            else:\n",
    "                                model.add(Dense(units[i], activation[i]))\n",
    "                        model.add(Dense(1, \"sigmoid\"))\n",
    "                        print(model.summary())\n",
    "\n",
    "                if st.button(\"Train\"):\n",
    "                    model.compile(\n",
    "                        loss=tf.keras.losses.binary_crossentropy,\n",
    "                        optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "                        metrics=[\"accuracy\"],\n",
    "                    )\n",
    "                    reduce_lr = ReduceLROnPlateau(\n",
    "                        monitor=\"val_loss\",\n",
    "                        factor=0.3,\n",
    "                        verbose=1,\n",
    "                        patience=10,\n",
    "                        min_lr=0.0000000001,\n",
    "                    )\n",
    "                    early_stopping_cb = EarlyStopping(\n",
    "                        patience=10, restore_best_weights=True\n",
    "                    )\n",
    "                    my_bar = st.progress(0)\n",
    "\n",
    "                    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "                        def on_epoch_end(self, epoch, logs=None):\n",
    "                            global percent\n",
    "                            percent += 1 / epochs\n",
    "                            if percent > 1:\n",
    "                                percent = 1\n",
    "                            my_bar.progress(percent)\n",
    "\n",
    "                    history_1 = model.fit(\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping_cb, reduce_lr, CustomCallback()],\n",
    "                    )\n",
    "\n",
    "                    plot_loss_curves(history_1)\n",
    "\n",
    "                    nn_f1 = evaluate(\"nn\", model, X_train, y_train, X_test, y_test)\n",
    "                    st.write(\"Model trained with an F1 score of\", nn_f1)\n",
    "\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "choice = st.selectbox(\n",
    "    \"Which models do you want to run?\",\n",
    "    [\n",
    "        \"\",\n",
    "        \"Feature Engineered\",\n",
    "        \"Feature Engineered + Scaling\",\n",
    "        \"Feature Engineered + SMOTE\",\n",
    "        \"Feature Engineered + SMOTE + Scaling\",\n",
    "        \"Feature Engineered + PCA  + SMOTE\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "if len(choice) > 0:\n",
    "    if choice == \"Feature Engineered\":\n",
    "        d = {\n",
    "            \"Logistic Regression\": {\"C\": 1.0, \"solver\": \"liblinear\"},\n",
    "            \"SVM\": {\"C\": 1.0, \"gamma\": 0.001, \"kernel\": \"rbf\"},\n",
    "            \"Random Forest\": {\n",
    "                \"max_features\": 0.25,\n",
    "                \"min_samples_split\": 6,\n",
    "                \"n_estimators\": 250,\n",
    "            },\n",
    "            \"KNN\": {\"n\": 8},\n",
    "            \"xgboost\": {\n",
    "                \"booster\": \"gbtree\",\n",
    "                \"colsample_bytree\": 0.6,\n",
    "                \"learning_rate\": 0.5,\n",
    "                \"max_depth\": 2,\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"n_estimators\": 9,\n",
    "            },\n",
    "            \"Lightboost\": {\n",
    "                \"colsample_bytree\": 0.5,\n",
    "                \"learning_rate\": 0.2,\n",
    "                \"max_depth\": 5,\n",
    "                \"n_estimators\": 50,\n",
    "                \"num_leaves\": 4,\n",
    "                \"reg_lambda\": 15,\n",
    "                \"scale_pos_weight\": 3,\n",
    "                \"subsample\": 0.9,\n",
    "            },\n",
    "        }\n",
    "        run(X_train, y_train, X_test, y_test, d)\n",
    "\n",
    "    elif choice == \"Feature Engineered + Scaling\":\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = X_train.copy()\n",
    "        X_test_std = X_test.copy()\n",
    "        X_train_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_train_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        X_test_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_test_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        d = {\n",
    "            \"Logistic Regression\": {\"C\": 1000.0, \"solver\": \"liblinear\"},\n",
    "            \"SVM\": {\"C\": 10, \"gamma\": 0.01, \"kernel\": \"rbf\"},\n",
    "            \"Random Forest\": {\n",
    "                \"max_features\": 0.25,\n",
    "                \"min_samples_split\": 6,\n",
    "                \"n_estimators\": 350,\n",
    "            },\n",
    "            \"KNN\": {\"n\": 8},\n",
    "            \"xgboost\": {\n",
    "                \"booster\": \"gbtree\",\n",
    "                \"colsample_bytree\": 0.6,\n",
    "                \"learning_rate\": 0.5,\n",
    "                \"max_depth\": 2,\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"n_estimators\": 9,\n",
    "            },\n",
    "            \"Lightboost\": {\n",
    "                \"colsample_bytree\": 0.5,\n",
    "                \"learning_rate\": 0.2,\n",
    "                \"max_depth\": 5,\n",
    "                \"n_estimators\": 50,\n",
    "                \"num_leaves\": 4,\n",
    "                \"reg_lambda\": 15,\n",
    "                \"scale_pos_weight\": 3,\n",
    "                \"subsample\": 0.9,\n",
    "            },\n",
    "        }\n",
    "        run(X_train_std, y_train, X_test_std, y_test, d)\n",
    "\n",
    "    elif choice == \"Feature Engineered + SMOTE\":\n",
    "        sm = SMOTE(random_state=0)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
    "        d = {\n",
    "            \"Logistic Regression\": {\"C\": 100.0, \"solver\": \"liblinear\"},\n",
    "            \"SVM\": {\"C\": 1.0, \"gamma\": 0.1, \"kernel\": \"rbf\"},\n",
    "            \"Random Forest\": {\n",
    "                \"max_features\": \"sqrt\",\n",
    "                \"min_samples_split\": 2,\n",
    "                \"n_estimators\": 350,\n",
    "            },\n",
    "            \"KNN\": {\"n\": 8},\n",
    "            \"xgboost\": {\n",
    "                \"booster\": \"gbtree\",\n",
    "                \"colsample_bytree\": 1,\n",
    "                \"learning_rate\": 0.6,\n",
    "                \"max_depth\": 4,\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"n_estimators\": 9,\n",
    "            },\n",
    "            \"Lightboost\": {\n",
    "                \"colsample_bytree\": 0.5,\n",
    "                \"learning_rate\": 0.2,\n",
    "                \"max_depth\": 7,\n",
    "                \"n_estimators\": 100,\n",
    "                \"num_leaves\": 11,\n",
    "                \"reg_lambda\": 10,\n",
    "                \"scale_pos_weight\": 3,\n",
    "                \"subsample\": 0.9,\n",
    "            },\n",
    "        }\n",
    "        run(X_train_res, y_train_res, X_test, y_test, d)\n",
    "\n",
    "    elif choice == \"Feature Engineered + SMOTE + Scaling\":\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = X_train.copy()\n",
    "        X_test_std = X_test.copy()\n",
    "        X_train_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_train_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        X_test_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_test_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        sm = SMOTE(random_state=0)\n",
    "        X_train_std_res, y_train_std_res = sm.fit_resample(X_train_std, y_train.ravel())\n",
    "        d = {\n",
    "            \"Logistic Regression\": {\"C\": 0.1, \"solver\": \"liblinear\"},\n",
    "            \"SVM\": {\"C\": 1000, \"gamma\": 0.0001, \"kernel\": \"rbf\"},\n",
    "            \"Random Forest\": {\n",
    "                \"max_features\": \"sqrt\",\n",
    "                \"min_samples_split\": 6,\n",
    "                \"n_estimators\": 350,\n",
    "            },\n",
    "            \"KNN\": {\"n\": 8},\n",
    "            \"xgboost\": {\n",
    "                \"booster\": \"gbtree\",\n",
    "                \"colsample_bytree\": 0.9,\n",
    "                \"learning_rate\": 0.5,\n",
    "                \"max_depth\": 4,\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"n_estimators\": 9,\n",
    "            },\n",
    "            \"Lightboost\": {\n",
    "                \"colsample_bytree\": 0.5,\n",
    "                \"learning_rate\": 0.2,\n",
    "                \"max_depth\": 5,\n",
    "                \"n_estimators\": 100,\n",
    "                \"num_leaves\": 10,\n",
    "                \"reg_lambda\": 25,\n",
    "                \"scale_pos_weight\": 3,\n",
    "                \"subsample\": 0.9,\n",
    "            },\n",
    "        }\n",
    "        run(X_train_std_res, y_train_std_res, X_test_std, y_test, d)\n",
    "    else:\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = X_train.copy()\n",
    "        X_test_std = X_test.copy()\n",
    "        X_train_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_train_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        X_test_std[\n",
    "            [\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]\n",
    "        ] = sc.fit_transform(\n",
    "            np.array(X_test_std[[\"Tenure Months\", \"Monthly Charges\", \"Total Charges\"]])\n",
    "        )\n",
    "        pca = PCA(0.8)\n",
    "        pca.fit(X_train_std)\n",
    "        X_train_pca = pca.transform(X_train_std)\n",
    "        X_test_pca = pca.transform(X_test_std)\n",
    "        sm = SMOTE(random_state=10)\n",
    "        X_train_std_res, y_train_std_res = sm.fit_resample(X_train_pca, y_train.ravel())\n",
    "        d = {\n",
    "            \"Logistic Regression\": {\"C\": 0.1, \"solver\": \"liblinear\"},\n",
    "            \"SVM\": {\"C\": 1000, \"gamma\": 0.0001, \"kernel\": \"rbf\"},\n",
    "            \"Random Forest\": {\n",
    "                \"max_features\": \"sqrt\",\n",
    "                \"min_samples_split\": 6,\n",
    "                \"n_estimators\": 350,\n",
    "            },\n",
    "            \"KNN\": {\"n\": 8},\n",
    "            \"xgboost\": {\n",
    "                \"booster\": \"gbtree\",\n",
    "                \"colsample_bytree\": 0.9,\n",
    "                \"learning_rate\": 0.5,\n",
    "                \"max_depth\": 4,\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"n_estimators\": 9,\n",
    "            },\n",
    "            \"Lightboost\": {\n",
    "                \"colsample_bytree\": 0.5,\n",
    "                \"learning_rate\": 0.2,\n",
    "                \"max_depth\": 5,\n",
    "                \"n_estimators\": 100,\n",
    "                \"num_leaves\": 10,\n",
    "                \"reg_lambda\": 25,\n",
    "                \"scale_pos_weight\": 3,\n",
    "                \"subsample\": 0.9,\n",
    "            },\n",
    "        }\n",
    "        run(X_train_std_res, y_train_std_res, X_test_pca, y_test, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
